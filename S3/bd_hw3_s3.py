# -*- coding: utf-8 -*-
"""BD HW3 S3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/138Kalrm1Kv2thvZB-J7vzfJNc_9JVC5j

# Step 3 (Work with DataFrame & Spark SQL)

## Environment Setup

### Install requirements
"""

!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz
!tar xf spark-3.1.1-bin-hadoop3.2.tgz
!pip install -q findspark

"""### Set environment variables"""

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.1.1-bin-hadoop3.2"

"""### Import libraries"""

import findspark
findspark.init()
from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local[*]").getOrCreate()
spark.conf.set("spark.sql.repl.eagerEval.enabled", True) # Property used to format output tables better
spark

import os
import pandas as pd
from google.colab import drive
from tqdm import tqdm

from pyspark.sql.functions import lit, col
from pyspark.sql.types import DoubleType, IntegerType, StringType, LongType
from pyspark.sql import functions as F
from pyspark.sql.window import Window

"""### Mount drive for log file"""

drive.mount('/content/drive')

"""## Make data ready

### Clean & Convert excel files to csv to be compatible with spark
"""

stocks_folder_path = '/content/drive/MyDrive/BD Stocks/xlsx'
stocks_csv_folder_path = '/content/drive/MyDrive/BD Stocks/csv'
for file_name in tqdm(os.listdir(stocks_folder_path)):
  file_path = os.path.join(stocks_folder_path, file_name)
  data = pd.read_excel(file_path)
  header = data.iloc[1]
  data = data.iloc[2:]
  data.columns = header
  csv_file_name = file_name.split(".")[0] + ".csv"
  csv_path = os.path.join(stocks_csv_folder_path, csv_file_name)
  data.to_csv(csv_path)

"""### Make dataframe"""

is_first = True
for file_name in tqdm(os.listdir(stocks_csv_folder_path)):
  file_path = os.path.join(stocks_csv_folder_path, file_name)
  dataframe = spark.read.csv(file_path, header=True, sep=",")
  year, month, day = file_name.split('.')[0].split('_')[2:]
  dataframe = dataframe.withColumn("day", lit(day))
  dataframe = dataframe.withColumn("month", lit(month))
  dataframe = dataframe.withColumn("year", lit(year))
  dataframe = dataframe.withColumn('بیشترین', F.col('بیشترین').cast(IntegerType()))
  dataframe = dataframe.withColumn('کمترین', F.col('کمترین').cast(IntegerType()))
  dataframe = dataframe.withColumn('قیمت پایانی - درصد', F.col('قیمت پایانی - درصد').cast(DoubleType()))
  dataframe = dataframe.withColumn('قیمت پایانی - تغییر', F.col('قیمت پایانی - تغییر').cast(DoubleType()))
  dataframe = dataframe.withColumn('قیمت پایانی - مقدار', F.col('قیمت پایانی - مقدار').cast(IntegerType()))
  dataframe = dataframe.withColumn('آخرین معامله - درصد', F.col('آخرین معامله - درصد').cast(DoubleType()))
  dataframe = dataframe.withColumn('آخرین معامله - تغییر', F.col('آخرین معامله - تغییر').cast(DoubleType()))
  dataframe = dataframe.withColumn('آخرین معامله - مقدار', F.col('آخرین معامله - مقدار').cast(IntegerType()))
  dataframe = dataframe.withColumn('اولین', F.col('اولین').cast(IntegerType()))
  dataframe = dataframe.withColumn('دیروز', F.col('دیروز').cast(IntegerType()))
  dataframe = dataframe.withColumn('ارزش', F.col('ارزش').cast(LongType()))
  dataframe = dataframe.withColumn('حجم', F.col('حجم').cast(LongType()))
  dataframe = dataframe.withColumn('تعداد', F.col('تعداد').cast(LongType()))
  dataframe = dataframe.withColumn('دیروز', F.col('دیروز').cast(LongType()))
  dataframe = dataframe.withColumnRenamed('بیشترین', 'max_price') \
                        .withColumnRenamed('کمترین', 'min_price') \
                        .withColumnRenamed('قیمت پایانی - درصد', 'close_price_change_percent') \
                        .withColumnRenamed('قیمت پایانی - تغییر', 'close_price_change') \
                        .withColumnRenamed('قیمت پایانی - مقدار', 'close_price') \
                        .withColumnRenamed('آخرین معامله - درصد', 'last_order_value_change_percent') \
                        .withColumnRenamed('آخرین معامله - تغییر', 'last_order_value_change') \
                        .withColumnRenamed('آخرین معامله - مقدار', 'last_order_value') \
                        .withColumnRenamed('اولین', 'first_order_value') \
                        .withColumnRenamed('ارزش', 'value') \
                        .withColumnRenamed('دیروز', 'yesterday_qnt') \
                        .withColumnRenamed('حجم', 'volume') \
                        .withColumnRenamed('تعداد', 'quantity') \
                        .withColumnRenamed('نام', 'full_name') \
                        .withColumnRenamed('نماد', 'symbol')
  dataframe = dataframe.drop('_c0')
  if is_first:
    final_df = dataframe
    is_first = False
  else:
    final_df = final_df.union(dataframe)
print(f'Final dataframe rows count: {final_df.cache().count()}')
print('First 5 rows:')
final_df.show(5)

"""### Make temporary table for sql queries"""

final_df.registerTempTable('stocks')

"""## Part 1

### With Dataframe

#### Extract last day dataframe
"""

last_day_df = final_df.filter((final_df['day']==27) & 
                              (final_df['month']==12) & 
                              (final_df['year']==1399))
last_day_df.cache().show(5)
print(f"Final day record count: {last_day_df.count()}")

"""#### Most expensive"""

most_expensive = last_day_df.orderBy('close_price', ascending=False).limit(10)
print("10 Most expensive symbols:")
most_expensive.select(most_expensive['symbol'], most_expensive['close_price']).show()

"""#### Cheapest"""

most_expensive = last_day_df.orderBy('close_price').limit(10)
print("10 Cheapest symbols:")
most_expensive.select(most_expensive['symbol'], most_expensive['close_price']).show()

"""### With Spark SQL

#### Most Expensive
"""

print("10 Most expensive symbols:")
spark.sql("SELECT symbol, close_price " +
          "FROM stocks " +
          "WHERE (year == 1399 AND month==12 AND day==27) " +
          "ORDER BY close_price DESC " +
          "LIMIT 10;").show()

"""#### Cheapest"""

print("10 Cheapest symbols:")
spark.sql("SELECT symbol, close_price " +
          "FROM stocks " +
          "WHERE (year == 1399 AND month==12 AND day==27) " +
          "ORDER BY close_price ASC " +
          "LIMIT 10;").show()

"""## Part 2

### With Dataframe
"""

most_traded = final_df.orderBy('volume', ascending=False).limit(1)
print("Most traded symbol:")
most_traded.select(most_traded['symbol'], most_traded['volume']).show()

"""### With Spark SQL"""

print("Most traded symbol:")
spark.sql("SELECT symbol, volume " +
          "FROM stocks " +
          "ORDER BY volume DESC " +
          "LIMIT 1;").show()

"""## Part 3

### With Dataframe
"""

symbol_month_change = (final_df
                       .groupBy(['symbol', 'month'])
                       .agg(F.sum('close_price_change').alias('month_change')))

window_spec = Window().partitionBy(['month']).orderBy(F.desc('month_change'))
ranked_symbols = symbol_month_change.withColumn("rank",F.rank().over(window_spec))
print("Most price rised symbol in each month:")
most_rised = (ranked_symbols
              .filter(ranked_symbols['rank'] < 11))
most_rised.show()

"""### With Spark SQL"""

print("Most price rised symbol in each month:")
spark.sql("SELECT * " + 
          "FROM ( " + 
          "	SELECT symbol, month, month_change, " + 
          "	row_number() over (partition by month order by month_change desc) as symbol_rank " + 
          "	FROM ( " + 
          "		SELECT symbol, month, SUM(close_price_change) as month_change " + 
          "		FROM stocks  " + 
          "		GROUP BY symbol, month " + 
          "		) AS symbol_month_change " + 
          "	) AS ranked_symbols " + 
          "WHERE symbol_rank <= 10;").show()

"""## Part 4

### With Dataframe
"""

most_fall = (final_df
             .groupBy('symbol')
             .agg(F.sum('close_price_change').alias('six_month_change'))
             .sort('six_month_change')).limit(10)
print("Most price fall symbol in 6 month:")
most_fall.show()

"""### With Spark SQL"""

print("Most price fall symbol in 6 month:")
spark.sql("SELECT symbol, SUM(close_price_change) as period_change " +
          "FROM stocks " +
          "GROUP BY symbol " +
          "ORDER BY period_change ASC " +
          "LIMIT 10;").show()

"""## Part 5

### With Dataframe
"""

most_closed = final_df.groupBy('symbol').count().orderBy('count')
print("Most closed symbols:")
most_closed.show()

"""### With Spark SQL"""

print("Most closed symbols:")
spark.sql("SELECT symbol, COUNT(*) as open_days " +
          "FROM stocks " +
          "GROUP BY symbol " +
          "ORDER BY open_days ASC;").show()